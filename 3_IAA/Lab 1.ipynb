{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Laboratorio 1: Regresión en California\n",
    "\n",
    "En este laboratorio deben hacer experimentos de regresión con el conjunto de datos \"California Housing dataset\".\n",
    "\n",
    "Estudiarán el dataset, harán visualizaciones y seleccionarán atributos relevantes a mano.\n",
    "\n",
    "Luego, entrenarán y evaluarán diferentes tipos de regresiones, buscando las configuraciones que mejores resultados den."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Carga del Conjunto de Datos\n",
    "\n",
    "Cargamos el conjunto de datos y vemos su contenido."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "X_california, y_california = fetch_california_housing(return_X_y=True, as_frame=True)\n",
    "california = fetch_california_housing()\n",
    "target_feature = \"MedHouseVal\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check data types and count nulls \n",
    "#print(X_california.dtypes)\n",
    "X_california.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_california.describe() # Average target_feature in $100,000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "california.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove rows that have atleast one row that deviates more than 3 standard deviations from the mean\n",
    "\n",
    "# Calculate the mean and standard deviation of each column\n",
    "mean = X_california.mean(axis=0)\n",
    "std = X_california.std(axis=0)\n",
    "\n",
    "# Calculate the z-score for each element in the matrix\n",
    "z = (X_california - mean) / std\n",
    "\n",
    "# Find the location of all elements that are more than 3 standard deviations away\n",
    "outliers = (np.abs(z) > 3).any(axis=1)\n",
    "\n",
    "\n",
    "# check data shape before removing outliers\n",
    "\n",
    "print(X_california.shape)\n",
    "print(y_california.shape)\n",
    "\n",
    "# Remove the corresponding rows from X and y\n",
    "\n",
    "X_california = X_california[~outliers]\n",
    "y_california = y_california[~outliers]\n",
    "\n",
    "# check data shape\n",
    "\n",
    "print(X_california.shape)\n",
    "print(y_california.shape)\n",
    "\n",
    "# Replace california['data'] and california['feature_names'] with X_california and y_california\n",
    "\n",
    "california['data'] = X_california.to_numpy()\n",
    "california['target'] = y_california.to_numpy()\n",
    "\n",
    "# check data shape\n",
    "\n",
    "print(california['data'].shape)\n",
    "print(california['target'].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(california['DESCR'])  # descripción del dataset\n",
    "#california['feature_names'] # nombres de los atributos para cada columna de 'data'\n",
    "#california['data']           # matriz con los datos de entrada (atributos)\n",
    "#print(california['target']) # vector de valores a predecir\n",
    "#california"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "california['data'].shape, california['target'].shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## División en Entrenamiento y Evaluación\n",
    "\n",
    "Dividimos aleatoriamente los datos en 80% para entrenamiento y 20% para evaluación:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X, y = california['data'], california['target']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.8, random_state=0)\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejercicio 1: Descripción de los Datos y la Tarea\n",
    "\n",
    "Responda las siguientes preguntas:\n",
    "\n",
    "1. ¿De qué se trata el conjunto de datos?\n",
    "2. ¿Cuál es la variable objetivo que hay que predecir? ¿Qué significado tiene?\n",
    "3. ¿Qué información (atributos) hay disponibles para hacer la predicción?\n",
    "4. ¿Qué atributos imagina ud. que serán los más determinantes para la predicción?\n",
    "5. ¿Qué problemas observa a priori en el conjunto de datos? ¿Observa posibles sesgos, riesgos, dilemas éticos, etc? Piense que los datos pueden ser utilizados para hacer predicciones futuras.\n",
    "\n",
    "**No hace falta escribir código para responder estas preguntas.**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Responder todas las preguntas acá.**\n",
    "1. El conjunto de datos California Housing procede del Censo de EE.UU. de 1990. Contiene información sobre las condiciones de vivienda en diferentes grupos de bloques de viviendas de California.\n",
    "\n",
    "Un **grupo de bloques** es la unidad geográfica más pequeña para la que la Oficina del Censo de EE.UU. publica datos.\n",
    "\n",
    "2. La variable objetivo que debe predecirse es la mediana de los precios de las viviendas en los grupos de bloques. Se expresa en cientos de miles de dólares (100.000 $). Representa el punto de precio en el que la mitad de las casas se valoran por encima de él y la otra mitad por debajo.\n",
    "\n",
    "3. Posee 20.640 registros con 8 atributos numéricos:\n",
    "\n",
    "  - ingreso medio\n",
    "  - la edad de la casa\n",
    "  - el promedio de habitaciones\n",
    "  - el promedio de dormitorios por hogar\n",
    "  - cantidad de habitantes en el bloque\n",
    "  - el promedio de miembros del hogar\n",
    "  - la latitud y la longitud de cada grupo de bloques.\n",
    "\n",
    "4. Probablemente los atributos más determinantes sean:\n",
    "    - **ingreso medio:** si hay un ingreso medio alto en el bloque, es probablemente un lugar donde gente con muchos recursos eligen vivir y gente con muchos recursos suelen vivir en lugares caros y análogamente con ingresos medio bajo.\n",
    "    - **promedio de habitaciones:** las casas con más habitaciones, siendo todas las demás variables iguales, suelen ser más caras que casas con menos habitaciones.\n",
    "    - **edad de la casa:** probablemente las casas viejas tengan una valuación menor que las nuevas\n",
    " \n",
    "5. Sí, hay que tener en cuenta las siguientes cuestiones\n",
    "\n",
    "- Los datos son de 1990, y las cosas pueden haber cambiado considerablemente en california desde entonces."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejercicio 2: Visualización de los Datos\n",
    "\n",
    "1. Para cada atributo de entrada, haga una gráfica que muestre su relación con la variable objetivo.\n",
    "2. Estudie las gráficas, identificando **a ojo** los atributos que a su criterio sean los más informativos para la predicción.\n",
    "3. Para ud., ¿cuáles son esos atributos? Lístelos en orden de importancia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1\n",
    "\"\"\"\n",
    "        - MedInc        median income in block group\n",
    "        - HouseAge      median house age in block group\n",
    "        - AveRooms      average number of rooms per household\n",
    "        - AveBedrms     average number of bedrooms per household\n",
    "        - Population    block group population\n",
    "        - AveOccup      average number of household members\n",
    "        - Latitude      block group latitude\n",
    "        - Longitude     block group longitude\n",
    "\"\"\"\n",
    "\n",
    "fig, axes = plt.subplots(2, 4, figsize=(20, 9))\n",
    "X_california_plot = X_california.sample(1000, random_state=0)\n",
    "y_california_plot = y_california.sample(1000, random_state=0)\n",
    "\n",
    "for i, ax in enumerate(axes.ravel()):\n",
    "    feature = X_california_plot.columns[i]\n",
    "    ax.scatter(X_california_plot[feature], y_california_plot, facecolor=\"dodgerblue\", edgecolor=\"k\", label=\"datos\", alpha=0.5)\n",
    "    ax.set_title(feature)\n",
    "    ax.set_ylabel(f\"{target_feature} (x$100,000)\")\n",
    "    ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 y 3\n",
    "<!--\n",
    "1. MedInc parece ser buen predictor\n",
    "2. un poco menos pero aún util, parece ser AveOccup\n",
    "3. si bien definitivamente no se correlacionan linealmente con el precio, Latitude y Longitude parecen aportar información que median income no tiene\n",
    "--> \n",
    "Podemos observar que los atributos que parecen ser buenos predictores importancia son `MedInc` y `AveRooms`recordemos que se conresponden con el ingreso medio del bloque y el promedio de habitaciones por hogar. Si bien no se correlacionan linelamente con el atributo que queremos predecir, `Latitude` y `Longitude` parecen aportar información que `MedInc` no refleja. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matriz de correlación\n",
    "\n",
    "df_with_target = X_california.copy()\n",
    "df_with_target[target_feature] = y_california\n",
    "corr = df_with_target.corr()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "sns.heatmap(corr, annot=True, cmap=\"Blues\", vmin=-1, vmax=1, ax=ax)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejercicio 3: Regresión Lineal\n",
    "\n",
    "1. Seleccione **un solo atributo** que considere puede ser el más apropiado.\n",
    "2. Instancie una regresión lineal de **scikit-learn**, y entrénela usando sólo el atributo seleccionado.\n",
    "3. Evalúe, calculando error cuadrático medio para los conjuntos de entrenamiento y evaluación.\n",
    "4. Grafique el modelo resultante, junto con los puntos de entrenamiento y evaluación.\n",
    "5. Interprete el resultado, haciendo algún comentario sobre las cualidades del modelo obtenido.\n",
    "\n",
    "**Observación:** Con algunos atributos se puede obtener un error en test menor a 50."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Resolver acá. Ayuda:\n",
    "feature = 'MedInc'\n",
    "selector = california['feature_names'].index(feature)\n",
    "\n",
    "\n",
    "X_train_f = X_train[:, selector] # selecciona la columna 'MedInc' de X_train\n",
    "X_test_f = X_test[:, selector]  # selecciona la columna 'MedInc' de X_test\n",
    "X_train_f.shape, X_test_f.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Instanciar y entrenar acá.\n",
    "from sklearn.linear_model import LinearRegression\n",
    "model = LinearRegression()\n",
    "model.fit(X_train_f.reshape(-1, 1), y_train) # reshape(-1, 1) convierte un vector fila en un vector columna\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Predecir y evaluar acá.\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "y_pred = model.predict(X_test_f.reshape(-1, 1))\n",
    "test_error = mean_squared_error(y_test, y_pred)\n",
    "\n",
    "print(f\"Test error: {test_error:.4f} x$100,000\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Graficar acá. Ayuda:\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "\n",
    "ax.scatter(X_test_f, y_test, facecolor=\"dodgerblue\", edgecolor=\"k\", label=\"datos\", alpha=0.5)\n",
    "ax.plot(X_test_f, y_pred, color=\"red\", label=\"modelo\")\n",
    "ax.set_xlabel(feature)\n",
    "ax.set_ylabel(f\"target_feature (x$100,000)\")\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "**5. Responder acá**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejercicio 4: Regresión Polinomial\n",
    "\n",
    "En este ejercicio deben entrenar regresiones polinomiales de diferente complejidad, siempre usando **scikit-learn**.\n",
    "\n",
    "Deben usar **el mismo atributo** seleccionado para el ejercicio anterior.\n",
    "\n",
    "1. Para varios grados de polinomio, haga lo siguiente:\n",
    "    1. Instancie y entrene una regresión polinomial.\n",
    "    2. Prediga y calcule error en entrenamiento y evaluación. Imprima los valores.\n",
    "    3. Guarde los errores en una lista.\n",
    "2. Grafique las curvas de error en términos del grado del polinomio.\n",
    "3. Interprete la curva, identificando el punto en que comienza a haber sobreajuste, si lo hay.\n",
    "4. Seleccione el modelo que mejor funcione, y grafique el modelo conjuntamente con los puntos.\n",
    "5. Interprete el resultado, haciendo algún comentario sobre las cualidades del modelo obtenido.\n",
    "\n",
    "**Observación:** Con algunos atributos se pueden obtener errores en test menores a 40 e incluso a 35."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Resolver acá.\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "degrees = range(2, 25)\n",
    "train_errors = []\n",
    "test_errors = []\n",
    "\n",
    "for degree in degrees:\n",
    "    model_pol = make_pipeline(PolynomialFeatures(degree), LinearRegression())\n",
    "    model_pol.fit(X_train_f.reshape(-1, 1), y_train)\n",
    "    \n",
    "    y_train_pred = model_pol.predict(X_train_f.reshape(-1, 1))\n",
    "    y_test_pred = model_pol.predict(X_test_f.reshape(-1, 1))\n",
    "\n",
    "    train_error = mean_squared_error(y_train, y_train_pred)\n",
    "    test_error = mean_squared_error(y_test, y_test_pred)\n",
    "    \n",
    "    train_errors.append(train_error)\n",
    "    test_errors.append(test_error)\n",
    "    print(f\"Degree: {degree}, train error: {train_error:.4f}, test error: {test_error:.4f}, coefficients: {model_pol[-1].coef_}\")\n",
    "    #    print(f\"coefficients: {model_pol[-1].coef_}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Graficar curvas de error acá.\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "\n",
    "ax.plot(degrees, train_errors, label=\"train error\")\n",
    "ax.plot(degrees, test_errors, label=\"test error\")\n",
    "ax.set_xlabel(\"degree\")\n",
    "ax.set_ylabel(\"error\")\n",
    "ax.set_xlim(2, 25)\n",
    "ax.set_xticks(range(2, 25, 1))\n",
    "ax.legend()\n",
    "plt.show()\n",
    "\n",
    "# Diferencial de error entre train y test por cada grado de polinomio\n",
    "\n",
    "diff_errors = np.abs(np.array(train_errors) - np.array(test_errors))\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "ax.plot(degrees, diff_errors, label=\"train error - test error\")\n",
    "ax.set_xlabel(\"degree\")\n",
    "ax.set_ylabel(\"error\")\n",
    "ax.set_xlim(2, 25)\n",
    "ax.set_xticks(range(2, 25, 1))\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. Responder acá**\n",
    "\n",
    "El error que se obtiene en el conjunto de datos de testing no difiere mucho del error obtenido en el conjunto de datos de entrenamiento,por lo que no se puede decir que el modelo esté sobreajustado. Sin embargo, el error obtenido es bastante alto, por lo que se puede decir que el modelo en general no es muy bueno y empeora notablemente a partir del grado 21."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_errors.index(min(test_errors))+2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Reconstruir mejor modelo acá y graficar.\n",
    "# El mejor polinomio es el de grado 6, graficar ese polinomio\n",
    "\n",
    "degree = 6\n",
    "model = make_pipeline(PolynomialFeatures(degree), LinearRegression())\n",
    "model.fit(X_train_f.reshape(-1, 1), y_train)\n",
    "\n",
    "y_train_pred = model.predict(X_train_f.reshape(-1, 1))\n",
    "y_test_pred = model.predict(X_test_f.reshape(-1, 1))\n",
    "\n",
    "train_error = mean_squared_error(y_train, y_train_pred)\n",
    "test_error = mean_squared_error(y_test, y_test_pred)\n",
    "\n",
    "print(f\"Degree: {degree}, train error: {train_error:.4f}, test error: {test_error:.4f}\")\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "\n",
    "ax.scatter(X_test_f, y_test, facecolor=\"dodgerblue\", edgecolor=\"k\", label=\"datos\", alpha=0.5)\n",
    "ax.scatter(X_test_f, y_test_pred, color=\"red\", label=\"modelo\")\n",
    "ax.set_xlabel(feature)\n",
    "ax.set_ylabel(f\"target_feature (x$100,000)\")\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos ver que se reduce el error cambiando de un regresor lineal a uno polinomial. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejercicio 5: Regresión con más de un Atributo\n",
    "\n",
    "En este ejercicio deben entrenar regresiones que toman más de un atributo de entrada.\n",
    "\n",
    "1. Seleccione **dos o tres atributos** entre los más relevantes encontrados en el ejercicio 2.\n",
    "2. Repita el ejercicio anterior, pero usando los atributos seleccionados. No hace falta graficar el modelo final.\n",
    "3. Interprete el resultado y compare con los ejercicios anteriores. ¿Se obtuvieron mejores modelos? ¿Porqué?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regresor lineal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Resolver acá. Ayuda (con dos atributos):\n",
    "selector = (np.array(california['feature_names']) == 'MedInc') | (np.array(california['feature_names']) == 'AveRooms')\n",
    "\n",
    "X_train_fs = X_train[:, selector]\n",
    "X_test_fs = X_test[:, selector]\n",
    "X_train_fs.shape, X_test_fs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_fs[8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Instanciar y entrenar acá.\n",
    "from sklearn.linear_model import LinearRegression\n",
    "model = LinearRegression()\n",
    "model.fit(X_train_fs, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Predecir y evaluar acá.\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "y_pred = model.predict(X_test_fs)\n",
    "test_error = mean_squared_error(y_test, y_pred)\n",
    "\n",
    "print(f\"Test error: {test_error:.4f} x$100,000\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regresor ponomial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Resolver acá.\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "degrees = range(2,25)\n",
    "train_errors = []\n",
    "test_errors = []\n",
    "\n",
    "for degree in degrees:\n",
    "    model = make_pipeline(PolynomialFeatures(degree), LinearRegression())\n",
    "    model.fit(X_train_fs, y_train)\n",
    "    \n",
    "    y_train_pred = model.predict(X_train_fs)\n",
    "    y_test_pred = model.predict(X_test_fs)\n",
    "\n",
    "    train_error = mean_squared_error(y_train, y_train_pred)\n",
    "    test_error = mean_squared_error(y_test, y_test_pred)\n",
    "    \n",
    "    train_errors.append(train_error)\n",
    "    test_errors.append(test_error)\n",
    "    #print(f\"Degree: {degree}, train error: {train_error:.4f}, test error: {test_error:.4f}\")\n",
    "    # print coefficients\n",
    "    print(f\"coefficients: {model[-1].coef_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# 2. Graficar curvas de error acá.\n",
    "#hay que arreglar este grafico\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "\n",
    "ax.plot(degrees, train_errors, label=\"train error\")\n",
    "ax.plot(degrees, test_errors, label=\"test error\")\n",
    "ax.set_xlabel(\"degree\")\n",
    "ax.set_ylabel(\"error\")\n",
    "ax.set_xlim(0, 25)\n",
    "ax.set_xticks(range(0, 25, 1))\n",
    "ax.legend()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "\n",
    "ax.plot(degrees, train_errors, label=\"train error\")\n",
    "ax.plot(degrees, test_errors, label=\"test error\")\n",
    "ax.set_xlabel(\"degree\")\n",
    "ax.set_ylabel(\"error\")\n",
    "ax.set_xlim(2, 10)\n",
    "ax.set_xticks(range(2, 25, 1))\n",
    "plt.xlim(2,20)\n",
    "plt.ylim(0.4,0.8)\n",
    "ax.legend()\n",
    "plt.show() \n",
    "\n",
    "\n",
    "# Diferencial de error entre train y test por cada grado de polinomio\n",
    "\n",
    "diff_errors = np.abs(np.array(train_errors) - np.array(test_errors))\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "ax.plot(degrees, diff_errors, label=\"train error - test error\")\n",
    "ax.set_xlabel(\"degree\")\n",
    "ax.set_ylabel(\"error\")\n",
    "ax.set_xlim(0, 25)\n",
    "ax.set_xticks(range(0, 25, 1))\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_errors.index(min(test_errors))+2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Reconstruir mejor modelo acá y graficar.\n",
    "\n",
    "# El mejor polinomio es el de grado 7, graficar ese polinomio\n",
    "\n",
    "degree = 7\n",
    "model = make_pipeline(PolynomialFeatures(degree), LinearRegression())\n",
    "model.fit(X_train_fs, y_train)\n",
    "\n",
    "y_train_pred = model.predict(X_train_fs)\n",
    "y_test_pred = model.predict(X_test_fs)\n",
    "\n",
    "train_error = mean_squared_error(y_train, y_train_pred)\n",
    "test_error = mean_squared_error(y_test, y_test_pred)\n",
    "\n",
    "print(f\"Degree: {degree}, train error: {train_error:.4f}, test error: {test_error:.4f}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cambiando el modelo de regresión lineal a una regresión polinomial y agregar el atributo `AveRooms`, obtuvimos una reducción mayor del error."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Más ejercicios (opcionales)\n",
    "\n",
    "### Ejercicio 6: A Todo Feature\n",
    "\n",
    "Entrene y evalúe regresiones pero utilizando todos los atributos de entrada (va a andar mucho más lento). Estudie los resultados.\n",
    "\n",
    "### Ejercicio 7: Regularización\n",
    "\n",
    "Entrene y evalúe regresiones con regularización \"ridge\". Deberá probar distintos valores de \"alpha\" (fuerza de la regularización). ¿Mejoran los resultados?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sin regularizacion\n",
    "\"\"\"\n",
    "Degree: 2, train error: 0.6893, test error: 0.7062\n",
    "Degree: 3, train error: 0.6891, test error: 0.7057\n",
    "Degree: 4, train error: 0.6886, test error: 0.7051\n",
    "Degree: 5, train error: 0.6857, test error: 0.7028\n",
    "Degree: 6, train error: 0.6842, test error: 0.7006\n",
    "Degree: 7, train error: 0.6842, test error: 0.7011\n",
    "Degree: 8, train error: 0.6841, test error: 0.7015\n",
    "Degree: 9, train error: 0.6841, test error: 0.7015\n",
    "Degree: 10, train error: 0.6840, test error: 0.7018\n",
    "Degree: 11, train error: 0.6839, test error: 0.7023\n",
    "Degree: 12, train error: 0.6839, test error: 0.7023\n",
    "Degree: 13, train error: 0.6839, test error: 0.7023\n",
    "Degree: 14, train error: 0.6838, test error: 0.7022\n",
    "Degree: 15, train error: 0.6838, test error: 0.7019\n",
    "Degree: 16, train error: 0.6843, test error: 0.7030\n",
    "Degree: 17, train error: 0.6850, test error: 0.7033\n",
    "Degree: 18, train error: 0.6853, test error: 0.7035\n",
    "Degree: 19, train error: 0.6861, test error: 0.7043\n",
    "Degree: 20, train error: 0.6880, test error: 0.7067\n",
    "Degree: 21, train error: 0.7042, test error: 0.7235\n",
    "Degree: 22, train error: 0.7129, test error: 0.7313\n",
    "Degree: 23, train error: 0.7228, test error: 0.7398\n",
    "Degree: 24, train error: 0.7337, test error: 0.7490\n",
    "\"\"\"\n",
    "\n",
    "# Regresion con regularizacion ridge (1 feature)\n",
    "\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "degrees = range(1, 7) # Grados mas altos causan Ill-conditioned matrix error y errores numericos\n",
    "alphas = [0, 0.01, 0.1, 1, 10, 100, 1000]\n",
    "train_errors = []\n",
    "test_errors = []\n",
    "\n",
    "results_table = np.zeros((len(degrees), len(alphas)), dtype=np.float64)\n",
    "\n",
    "for i, degree in enumerate(degrees):\n",
    "    for j, alpha in enumerate(alphas):\n",
    "        model = make_pipeline(PolynomialFeatures(degree), Ridge(alpha=alpha))\n",
    "        model.fit(X_train_f.reshape(-1, 1), y_train)\n",
    "    \n",
    "        y_train_pred = model.predict(X_train_f.reshape(-1, 1))\n",
    "        y_test_pred = model.predict(X_test_f.reshape(-1, 1))\n",
    "\n",
    "        train_error = mean_squared_error(y_train, y_train_pred)\n",
    "        test_error = mean_squared_error(y_test, y_test_pred)\n",
    "    \n",
    "        train_errors.append(train_error)\n",
    "        test_errors.append(test_error)\n",
    "        results_table[i, j] = test_error\n",
    "        #print(f\"Coeficientes: {model[-1].coef_}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Graficar tabla de resultados acá.\n",
    "\n",
    "df_results = pd.DataFrame(results_table, columns=alphas)\n",
    "df_results[\"degree\"] = degrees\n",
    "df_results = df_results.set_index(\"degree\")\n",
    "df_results\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "- Como los coeficientes de las regresiones polinomiales no son muy grandes, es decir, no hay mucho sobreajuste, al regularizar no se obtienen mejoras significativas si es que las hay."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
